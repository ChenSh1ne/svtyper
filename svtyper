#!/usr/bin/env python

import pysam
import json
import argparse, sys
import math, time, re
from collections import Counter
from argparse import RawTextHelpFormatter

__author__ = "Colby Chiang (cc2qe@virginia.edu)"
__version__ = "$Revision: 0.0.4 $"
__date__ = "$Date: 2016-04-05 09:07 $"

# --------------------------------------
# README
#
# 1. orientations o1 and o2 are boolean where
# plus strand is True and minus strand is False

# --------------------------------------
# define functions

def get_args():
    parser = argparse.ArgumentParser(formatter_class=RawTextHelpFormatter, description="\
svtyper\n\
author: " + __author__ + "\n\
version: " + __version__ + "\n\
description: Compute genotype of structural variants based on breakpoint depth")
    parser.add_argument('-B', '--bam', type=str, required=True, help='BAM file(s), comma-separated if genotyping multiple BAMs')
    parser.add_argument('-i', '--input_vcf', type=argparse.FileType('r'), default=None, help='VCF input (default: stdin)')
    parser.add_argument('-o', '--output_vcf', type=argparse.FileType('w'), default=sys.stdout, help='output VCF to write (default: stdout)')
    # parser.add_argument('-f', '--splflank', type=int, required=False, default=20, help='min number of split read query bases flanking breakpoint on either side [20]')
    # parser.add_argument('-F', '--discflank', type=int, required=False, default=20, help='min number of discordant read query bases flanking breakpoint on either side. (should not exceed read length) [20]')
    parser.add_argument('-m', '--min_aligned', type=int, required=False, default=20, help='minimum number of aligned bases to consider read as evidence [20]')
    parser.add_argument('--split_weight', type=float, required=False, default=1, help='weight for split reads [1]')
    parser.add_argument('--disc_weight', type=float, required=False, default=1, help='weight for discordant paired-end reads [1]')
    parser.add_argument('-n', dest='num_samp', type=int, required=False, default=1000000, help='number of pairs to sample from BAM file for building insert size distribution [1000000]')
    # parser.add_argument('-M', action='store_true', dest='legacy', required=False, help='split reads are flagged as secondary, not supplementary. For compatibility with legacy BWA-MEM "-M" flag')
    parser.add_argument('-l', '--lib_info', dest='lib_info_file', type=argparse.FileType('r'), required=False, default=None, help='JSON file of library information')
    parser.add_argument('--debug', action='store_true', help='debugging verbosity')
    parser.add_argument('--dump', type=str, required=False, default=None, help='file prefix to dump library info and relevant BAM reads')

    # parse the arguments
    args = parser.parse_args()

    # if no input, check if part of pipe and if so, read stdin.
    if args.input_vcf == None:
        if sys.stdin.isatty():
            parser.print_help()
            exit(1)
        else:
            args.input_vcf = sys.stdin
    # send back the user input
    return args

class Vcf(object):
    def __init__(self):
        self.file_format = 'VCFv4.2'
        # self.fasta = fasta
        self.reference = ''
        self.sample_list = []
        self.info_list = []
        self.format_list = []
        self.alt_list = []
        self.add_format('GT', 1, 'String', 'Genotype')

    def add_header(self, header):
        for line in header:
            if line.split('=')[0] == '##fileformat':
                self.file_format = line.rstrip().split('=')[1]
            elif line.split('=')[0] == '##reference':
                self.reference = line.rstrip().split('=')[1]
            elif line.split('=')[0] == '##INFO':
                a = line[line.find('<')+1:line.find('>')]
                r = re.compile(r'(?:[^,\"]|\"[^\"]*\")+')
                self.add_info(*[b.split('=')[1] for b in r.findall(a)])
            elif line.split('=')[0] == '##ALT':
                a = line[line.find('<')+1:line.find('>')]
                r = re.compile(r'(?:[^,\"]|\"[^\"]*\")+')
                self.add_alt(*[b.split('=')[1] for b in r.findall(a)])
            elif line.split('=')[0] == '##FORMAT':
                a = line[line.find('<')+1:line.find('>')]
                r = re.compile(r'(?:[^,\"]|\"[^\"]*\")+')
                self.add_format(*[b.split('=')[1] for b in r.findall(a)])
            elif line[0] == '#' and line[1] != '#':
                self.sample_list = line.rstrip().split('\t')[9:]

    # return the VCF header
    def get_header(self):
        header = '\n'.join(['##fileformat=' + self.file_format,
                            '##fileDate=' + time.strftime('%Y%m%d'),
                            '##reference=' + self.reference] + \
                           [i.hstring for i in self.info_list] + \
                           [a.hstring for a in self.alt_list] + \
                           [f.hstring for f in self.format_list] + \
                           ['\t'.join([
                               '#CHROM',
                               'POS',
                               'ID',
                               'REF',
                               'ALT',
                               'QUAL',
                               'FILTER',
                               'INFO',
                               'FORMAT'] + \
                                      self.sample_list
                                  )])
        return header

    def add_info(self, id, number, type, desc):
        if id not in [i.id for i in self.info_list]:
            inf = self.Info(id, number, type, desc)
            self.info_list.append(inf)

    def add_alt(self, id, desc):
        if id not in [a.id for a in self.alt_list]:
            alt = self.Alt(id, desc)
            self.alt_list.append(alt)

    def add_format(self, id, number, type, desc):
        if id not in [f.id for f in self.format_list]:
            fmt = self.Format(id, number, type, desc)
            self.format_list.append(fmt)

    def add_sample(self, name):
        self.sample_list.append(name)

    # get the VCF column index of a sample
    # NOTE: this is zero-based, like python arrays
    def sample_to_col(self, sample):
        return self.sample_list.index(sample) + 9

    class Info(object):
        def __init__(self, id, number, type, desc):
            self.id = str(id)
            self.number = str(number)
            self.type = str(type)
            self.desc = str(desc)
            # strip the double quotes around the string if present
            if self.desc.startswith('"') and self.desc.endswith('"'):
                self.desc = self.desc[1:-1]
            self.hstring = '##INFO=<ID=' + self.id + ',Number=' + self.number + ',Type=' + self.type + ',Description=\"' + self.desc + '\">'

    class Alt(object):
        def __init__(self, id, desc):
            self.id = str(id)
            self.desc = str(desc)
            # strip the double quotes around the string if present
            if self.desc.startswith('"') and self.desc.endswith('"'):
                self.desc = self.desc[1:-1]
            self.hstring = '##ALT=<ID=' + self.id + ',Description=\"' + self.desc + '\">'

    class Format(object):
        def __init__(self, id, number, type, desc):
            self.id = str(id)
            self.number = str(number)
            self.type = str(type)
            self.desc = str(desc)
            # strip the double quotes around the string if present
            if self.desc.startswith('"') and self.desc.endswith('"'):
                self.desc = self.desc[1:-1]
            self.hstring = '##FORMAT=<ID=' + self.id + ',Number=' + self.number + ',Type=' + self.type + ',Description=\"' + self.desc + '\">'

class Variant(object):
    def __init__(self, var_list, vcf):
        self.chrom = var_list[0]
        self.pos = int(var_list[1])
        self.var_id = var_list[2]
        self.ref = var_list[3]
        self.alt = var_list[4]
        if var_list[5] == '.':
            self.qual = 0
        else:
            self.qual = float(var_list[5])
        self.filter = var_list[6]
        self.sample_list = vcf.sample_list
        self.info_list = vcf.info_list
        self.info = dict()
        self.format_list = vcf.format_list
        self.active_formats = list()
        self.gts = dict()

        # fill in empty sample genotypes
        if len(var_list) < 8:
            sys.stderr.write('\nError: VCF file must have at least 8 columns\n')
            exit(1)
        if len(var_list) < 9:
            var_list.append("GT")

        # make a genotype for each sample at variant
        for s in self.sample_list:
            try:
                s_gt = var_list[vcf.sample_to_col(s)].split(':')[0]
                self.gts[s] = Genotype(self, s, s_gt)
                # import the existing fmt fields
                for j in zip(var_list[8].split(':'), var_list[vcf.sample_to_col(s)].split(':')):
                    self.gts[s].set_format(j[0], j[1])
            except IndexError:
                self.gts[s] = Genotype(self, s, './.')

        self.info = dict()
        i_split = [a.split('=') for a in var_list[7].split(';')] # temp list of split info column
        for i in i_split:
            if len(i) == 1:
                i.append(True)
            self.info[i[0]] = i[1]

    def set_info(self, field, value):
        if field in [i.id for i in self.info_list]:
            self.info[field] = value
        else:
            sys.stderr.write('\nError: invalid INFO field, \"' + field + '\"\n')
            exit(1)

    def get_info(self, field):
        return self.info[field]

    def get_info_string(self):
        i_list = list()
        for info_field in self.info_list:
            if info_field.id in self.info.keys():
                if info_field.type == 'Flag':
                    i_list.append(info_field.id)
                else:
                    i_list.append('%s=%s' % (info_field.id, self.info[info_field.id]))
        return ';'.join(i_list)

    def get_format_string(self):
        f_list = list()
        for f in self.format_list:
            if f.id in self.active_formats:
                f_list.append(f.id)
        return ':'.join(f_list)

    def genotype(self, sample_name):
        if sample_name in self.sample_list:
            return self.gts[sample_name]
        else:
            sys.stderr.write('\nError: invalid sample name, \"' + sample_name + '\"\n')

    def get_var_string(self):
        s = '\t'.join(map(str,[
            self.chrom,
            self.pos,
            self.var_id,
            self.ref,
            self.alt,
            '%0.2f' % self.qual,
            self.filter,
            self.get_info_string(),
            self.get_format_string(),
            '\t'.join(self.genotype(s).get_gt_string() for s in self.sample_list)
        ]))
        return s

class Genotype(object):
    def __init__(self, variant, sample_name, gt):
        self.format = dict()
        self.variant = variant
        self.set_format('GT', gt)

    def set_format(self, field, value):
        if field in [i.id for i in self.variant.format_list]:
            self.format[field] = value
            if field not in self.variant.active_formats:
                self.variant.active_formats.append(field)
                # sort it to be in the same order as the format_list in header
                self.variant.active_formats.sort(key=lambda x: [f.id for f in self.variant.format_list].index(x))
        else:
            sys.stderr.write('\nError: invalid FORMAT field, \"' + field + '\"\n')
            exit(1)

    def get_format(self, field):
        return self.format[field]

    def get_gt_string(self):
        g_list = list()
        for f in self.variant.active_formats:
            if f in self.format:
                if type(self.format[f]) == float:
                    g_list.append('%0.2f' % self.format[f])
                else:
                    g_list.append(self.format[f])
            else:
                g_list.append('.')
        return ':'.join(map(str,g_list))

# write read to BAM file, checking whether read is already written
def write_read(read, bam, written_reads):
    read.seq = "*"
    read.qual = "*"

    read_hash = (read.query_name, read.flag, read.cigarstring)

    if bam is None:
        return written_reads
    if read_hash in written_reads:
        return written_reads
    else:
        # print read.query_name, read.flag
        # print read_hash
        # print written_reads
        
        bam.write(read)
        written_reads.add(read_hash)
        return written_reads

# efficient combinatorial function to handle extremely large numbers
def log_choose(n, k):
    r = 0.0
    # swap for efficiency if k is more than half of n
    if k * 2 > n:
        k = n - k

    for  d in xrange(1,k+1):
        r += math.log(n, 10)
        r -= math.log(d, 10)
        n -= 1

    return r

# return the genotype and log10 p-value
def bayes_gt(ref, alt, is_dup):
    # probability of seeing an alt read with true genotype of of hom_ref, het, hom_alt respectively
    if is_dup: # specialized logic to handle non-destructive events such as duplications
        p_alt = [0.01, 0.3, 0.5]
    else:
        p_alt = [0.01, 0.5, 0.9]

    total = ref + alt

    lp_homref = log_choose(total, alt) + alt * math.log(p_alt[0], 10) + ref * math.log(1 - p_alt[0], 10)
    lp_het = log_choose(total, alt) + alt * math.log(p_alt[1], 10) + ref * math.log(1 - p_alt[1], 10)
    lp_homalt = log_choose(total, alt) + alt * math.log(p_alt[2], 10) + ref * math.log(1 - p_alt[2], 10)

    return (lp_homref, lp_het, lp_homalt)

# get the number of entries in the set
def countRecords(myCounter):
    numRecords = sum(myCounter.values())
    return numRecords

# median is approx 50th percentile, except when it is between
# two values in which case it's the mean of them.
def median(myCounter):
    #length is the number of bases we're looking at
    numEntries = countRecords(myCounter)

    # the ordinal value of the middle element
    # if 2 middle elements, then non-integer
    limit = 0.5 * numEntries

    # a list of the values, sorted smallest to largest
    # note that this list contains unique elements only
    valueList = list(myCounter)
    valueList.sort()

    # number of entries we've gone through
    runEntries = 0
    # index of the current value in valueList
    i = 0
    # initiate v, in case list only has one element
    v = valueList[i]

    # move through the value list, iterating by number of
    # entries for each value
    while runEntries < limit:
        v = valueList[i]
        runEntries += myCounter[v]
        i += 1
    if runEntries == limit:
        return (v + valueList[i]) / 2.0
    else:
        return v

# calculate upper median absolute deviation
def upper_mad(myCounter, myMedian):
    residCounter = Counter()
    for x in myCounter:
        if x > myMedian:
            residCounter[abs(x - myMedian)] += myCounter[x]
    return median(residCounter)

# sum of the entries
def sumRecords(myCounter):
    mySum = 0.0
    for c in myCounter:
        mySum += c * float(myCounter[c])
    return mySum

# calculate the arithmetic mean, given a counter and the
# length of the feature (chromosome or genome)
# for x percentile, x% of the elements in the set are
# <= the output value
def mean(myCounter):
    # the number of total entries in the set is the
    # sum of the occurrences for each value
    numRecords = countRecords(myCounter)

    # u holds the mean
    u = float()

    u = sumRecords(myCounter) / numRecords
    return u

def stdev(myCounter):
    # the number of total entries in the set is the
    # sum of the occurrences for each value
    numRecords = countRecords(myCounter)

    # u holds the mean
    u = mean(myCounter)
    sumVar = 0.0

    # stdev is sqrt(sum((x-u)^2)/#elements)
    for c in myCounter:
        sumVar += myCounter[c] * (c - u)**2
    myVariance = float(sumVar) / numRecords
    stdev = myVariance**(0.5)
    return stdev

# holds a library's insert size and read length information
class Library(object):
    def __init__(self,
                 name,
                 bam,
                 readgroups,
                 read_length,
                 hist,
                 dens,
                 mean,
                 sd,
                 prevalence,
                 num_samp):

        # parse arguments
        self.name = name
        self.bam = bam
        self.num_samp = num_samp
        self.readgroups = readgroups
        self.read_length = read_length
        self.hist = hist
        self.dens = dens
        self.mean = mean
        self.sd = sd
        self.prevalence = prevalence

        # if information is missing, compute it
        if self.read_length is None:
            self.calc_read_length()
        if self.hist is None:
            self.calc_insert_hist()
        if self.dens is None:
            self.calc_insert_density()
        if self.prevalence is None:
            self.calc_lib_prevalence()

    @classmethod
    def from_lib_info(cls,
                      sample_name,
                      lib_index,
                      bam,
                      lib_info):

        lib = lib_info[sample_name]['libraryArray'][lib_index]

        # convert the histogram keys to integers (from strings in JSON)
        lib_hist = {int(k):int(v) for k,v in lib['histogram'].items()}

        return cls(lib['library_name'],
                   bam,
                   lib['readgroups'],
                   int(lib['read_length']),
                   lib_hist,
                   None,
                   float(lib['mean']),
                   float(lib['sd']),
                   float(lib['prevalence']),
                   0)

    @classmethod
    def from_bam(cls,
                 lib_name,
                 bam,
                 num_samp):

        # get readgroups that comprise the library
        readgroups = []
        for r in bam.header['RG']:
            try:
                in_lib = r['LB'] == lib_name
            except KeyError, e:
                in_lib = lib_name == ''

            if in_lib:
                readgroups.append(r['ID'])

        return cls(lib_name,
                   bam,
                   readgroups,
                   None,
                   None,
                   None,
                   None,
                   None,
                   None,
                   num_samp)

    # calculate the library's prevalence in the BAM file
    def calc_lib_prevalence(self):
        max_count = 100000
        lib_counter = 0
        read_counter = 0

        for read in self.bam.fetch():
            if read_counter == max_count:
                break
            if read.get_tag('RG') in self.readgroups:
                lib_counter += 1
            read_counter += 1

        self.prevalence = float(lib_counter) / read_counter

    # get read length
    def calc_read_length(self):
        max_rl = 0
        counter = 0
        num_samp = 10000
        for read in self.bam.fetch():
            if read.get_tag('RG') not in self.readgroups:
                continue
            if read.infer_query_length() > max_rl:
                max_rl = read.infer_query_length()
            if counter == num_samp:
                break
            counter += 1
        self.read_length = max_rl

    # generate empirical histogram of the sample's insert size distribution
    def calc_insert_hist(self):
        counter = 0
        skip = 0
        skip_counter = 0
        mads = 10
        ins_list = []

        # Each entry in valueCounts is a value, and its count is
        # the number of instances of that value observed in the dataset.
        # So valueCount[5] is the number of times 5 has been seen in the data.
        valueCounts = Counter()
        for read in self.bam:
            if skip_counter < skip:
                skip_counter += 1
                continue
            if (read.is_reverse
                or not read.mate_is_reverse
                or read.is_unmapped
                or read.mate_is_unmapped
                or not is_primary(read)
                or read.template_length <= 0
                or read.get_tag('RG') not in self.readgroups):
                continue
            else:
                valueCounts[read.template_length] += 1
                counter += 1
            if counter == self.num_samp:
                break

        # remove outliers
        med = median(valueCounts)
        u_mad = upper_mad(valueCounts, med)
        for x in [x for x in list(valueCounts) if x > med + mads * u_mad]:
            del valueCounts[x]

        self.hist = valueCounts
        self.mean = mean(self.hist)
        self.sd = stdev(self.hist)

    # calculate the density curve for and insert size histogram
    def calc_insert_density(self):
        dens = Counter()
        for i in list(self.hist):
            dens[i] = float(self.hist[i])/countRecords(self.hist)
        self.dens = dens


# holds each sample's BAM and library information
class Sample(object):
    # general constructor
    def __init__(self,
                 name,
                 bam,
                 num_samp,
                 lib_dict,
                 rg_to_lib,
                 min_lib_prevalence):

        self.name = name
        self.bam = bam
        self.lib_dict = lib_dict
        self.rg_to_lib = rg_to_lib

        # get active libraries
        self.active_libs = []
        for lib in lib_dict.values():
            if lib.prevalence >= min_lib_prevalence:
                self.active_libs.append(lib)

    # constructor from supplied JSON descriptor
    @classmethod
    def from_lib_info(cls,
                      bam,
                      lib_info,
                      min_lib_prevalence):

        name = bam.header['RG'][0]['SM']
        num_samp = 0
        rg_to_lib = {}
        lib_dict = {}

        for i in xrange(len(lib_info[name]['libraryArray'])):
            lib = lib_info[name]['libraryArray'][i]
            lib_name = lib['library_name']

            # make library object
            lib_dict[lib_name] = Library.from_lib_info(name,
                                                       i,
                                                       bam,
                                                       lib_info)

            # make a map from readgroup IDs to library objects
            for rg in lib['readgroups']:
                rg_to_lib[rg] = lib_dict[lib_name]

        return cls(name,
                   bam,
                   num_samp,
                   lib_dict,
                   rg_to_lib,
                   min_lib_prevalence)

    # constructor for empirical distributions
    @classmethod
    def from_bam(cls,
                 bam,
                 num_samp,
                 min_lib_prevalence):

        name = bam.header['RG'][0]['SM']
        rg_to_lib = {}
        lib_dict = {}

        # self.lib_dict = dict()
        # self.active_libs = []
        # self.rg_to_lib = dict()

        for r in bam.header['RG']:
            try:
                lib_name=r['LB']
            except KeyError, e:
                lib_name=''

            # add the new library
            if lib_name not in lib_dict:
                new_lib = Library.from_bam(lib_name, bam, num_samp)
                lib_dict[lib_name] = new_lib
            rg_to_lib[r['ID']] = lib_dict[lib_name]

        return cls(name,
                   bam,
                   num_samp,
                   lib_dict,
                   rg_to_lib,
                   min_lib_prevalence)

    # get the maximum fetch flank for reading the BAM file
    def get_fetch_flank(self, z):
        return max([lib.mean + (lib.sd * z) for lib in self.lib_dict.values()])

    # return the library object for a specified read group
    def get_lib(self, readgroup):
        return self.rg_to_lib[readgroup]

# dump the sample and library info to a file
def write_sample_json(sample_list, lib_info_dump):
    lib_info = {}
    for sample in sample_list:
        s = {}
        s['sample_name'] = sample.name
        s['bam'] = sample.bam.filename
        s['libraryArray'] = []
        s['mapped'] = sample.bam.mapped
        s['unmapped'] = sample.bam.unmapped

        for lib in sample.lib_dict.values():
            l = {}
            l['library_name'] = lib.name
            l['readgroups'] = lib.readgroups
            l['read_length'] = lib.read_length
            l['mean'] = lib.mean
            l['sd'] = lib.sd
            l['prevalence'] = lib.prevalence
            l['histogram'] = lib.hist

            s['libraryArray'].append(l)

        lib_info[sample.name] = s

    # print the json file
    json.dump(lib_info, lib_info_dump, indent=4)
    lib_info_dump.close()

def min_non_overlap(splitA, splitB):
    if splitA.reference_id != splitB.reference_id:
        return 1000000000

    else:
        # this is how we've computed non-overlap in the past, but
        # it assumes that splitters are clipped on the right
        # and left side respectively. So it won't work for '--' inversions, for example
        overlap = min(splitA.reference_end, splitB.reference_end) - max(splitA.reference_start, splitB.reference_start)
        nonOverlapA = splitA.reference_end - splitA.reference_start - overlap
        nonOverlapB = splitB.reference_end - splitB.reference_start - overlap

        return min(nonOverlapA, nonOverlapB)

def read_overlaps(read, chrom, pos, min_aligned):
    if read.reference_name != chrom:
        return False
    return pos in read.positions[min_aligned:-min_aligned]

def is_split(readA, readB,
             chromA, posA,
             chromB, posB,
             o1, o2,
             split_slop,
             min_aligned,
             min_non_overlap = 20):

    # check chrom and strands
    if (readA.is_reverse and o1
        or readB.is_reverse and o2
        or readA.reference_name != chromA
        or readB.reference_name != chromB):
        return False

    # infer query alignment length from CIGAR
    readA_alignment_length = sum([x[1] for x in readA.cigartuples if (x[0] == 0 or x[0] == 7)])
    readB_alignment_length = sum([x[1] for x in readB.cigartuples if (x[0] == 0 or x[0] == 7)])
    if readA_alignment_length < min_aligned or readB_alignment_length < min_aligned:
        return False

    # check orientations and positions
    if o1 and readA.reference_end > posA + split_slop:
        return False
    elif not o1 and readA.reference_start < posA - split_slop:
        return False
    elif o2 and readB.reference_end > posB + split_slop:
        return False
    elif not o2 and readB.reference_start < posB - split_slop:
        return False

    else:
        return True

    # # check orientations and positions
    # if o1 and readA.reference_end <= posA + split_slop:
    #     return True
    # elif not o1 and readA.reference_start >= posA - split_slop:
    #     return True
    # elif o2 and readB.reference_end <= posB + split_slop:
    #     return True
    # elif not o2 and readB.reference_start >= posB - split_slop:
    #     return True

    # else:
    #     return False

# def is_split(readA, readB,
#              chromA, posA,
#              chromB, posB,
#              o1, o2,
#              split_slop,
#              min_aligned,
#              min_non_overlap = 20):

#     # check chrom and strands
#     if (readA.is_reverse and o1
#         or readB.is_reverse and o2
#         or readA.reference_name != chromA
#         or readB.reference_name != chromB):
#         return False

#     # infer query alignment length from CIGAR
#     readA_alignment_length = sum([x[1] for x in readA.cigartuples if (x[0] == 0 or x[0] == 7)])
#     readB_alignment_length = sum([x[1] for x in readB.cigartuples if (x[0] == 0 or x[0] == 7)])
#     if readA_alignment_length < min_aligned or readB_alignment_length < min_aligned:
#         return False

#     # check orientations and positions
#     if o1 and readA.reference_end > posA + split_slop:
#         return False
#     elif not o1 and readA.reference_start < posA - split_slop:
#         return False
#     elif o2 and readB.reference_end > posB + split_slop:
#         return False
#     elif not o2 and readB.reference_start < posB - split_slop:
#         return False

#     else:
#         return True

#     # # check orientations and positions
#     # if o1 and readA.reference_end <= posA + split_slop:
#     #     return True
#     # elif not o1 and readA.reference_start >= posA - split_slop:
#     #     return True
#     # elif o2 and readB.reference_end <= posB + split_slop:
#     #     return True
#     # elif not o2 and readB.reference_start >= posB - split_slop:
#     #     return True

#     # else:
#     #     return False

# read is neither secondary nor supplementary
def is_primary(read):
    return (not read.is_supplementary and not read.is_secondary)

# get the non-phred-scaled mapq of a read
def prob_mapq(read):
    return 1 - 10 ** (-read.mapq / 10.0)

# flip the strand
def flip_strand(strand):
    if strand == '+':
        return '-'
    elif strand == '-':
        return '+'

class SamFragment(object):
    def __init__(self, read, lib):
        self.lib = lib
        self.primary_reads = []
        self.auxil_reads = []
        self.read_set = set()
        self.num_primary = 0
        self.num_auxil = 0

        self.readA = None
        self.readB = None
        self.ispan = None
        self.ospan = None

        self.add_read(read)

    def group_reads(self):
        read_dict = {}
        for read in self.primary_reads + self.auxil_reads:
            read_sided = (read.query_name, read.is_read1)
            if read_sided in read_dict:
                read_dict[read_sided].append(read)
            else:
                read_dict[read_sided] = [read]
        return read_dict

    def add_read(self, read):
        # ensure we don't add the same read twice
        read_hash = read.__hash__()
        if read_hash in self.read_set:
            return
        else:
            self.read_set.add(read_hash)
        
        if is_primary(read):
            self.primary_reads.append(read)
            self.num_primary += 1

            # complete set of primaries
            if self.num_primary == 2:
                self.readA, self.readB = self.primary_reads
        else:
            self.auxil_reads.append(read)
            # self.num_auxil += 1

    def get_ispan(self, min_aligned):
        ispan1 = self.readA.reference_start + min_aligned
        ispan2 = self.readB.reference_end - min_aligned - 1

        return (ispan1, ispan2)

    def get_ospan(self):
        ospan1 = self.readA.reference_start
        ospan2 = self.readB.reference_end

        return (ospan1, ospan2)

    # returns boolean of whether the primary pair of a
    # fragment straddles a genomic point
    def is_pair_straddle(self,
                         chromA, posA, ciA,
                         chromB, posB, ciB,
                         o1, o2,
                         min_aligned):
        if self.num_primary != 2:
            return False
        
        # check orientation
        elif self.readA.is_reverse == o1:
            return False
        elif self.readB.is_reverse == o2:
            return False
        
        # check chromosome matching
        # Note: this step is kind of slow
        elif self.readA.reference_name != chromA:
            return False
        elif self.readB.reference_name != chromB:
            return False

        # get the inner span
        ispan = self.get_ispan(min_aligned)

        # check orientations and positions
        if o1 and ispan[0] > posA:
            return False
        elif not o1 and ispan[0] < posA:
            return False
        elif o2 and ispan[1] > posB:
            return False
        elif not o2 and ispan[1] < posB:
            return False

        else:
            return True
    
    # calculate the probability that a read pair is concordant at a breakpoint,
    # given the putative variant size and insert distribution of the library.
    def p_concordant(self, var_length=None):
        # a priori probability that a read-pair is concordant
        conc_prior = 0.95
        disc_prior = 1 - conc_prior

        ospan = self.get_ospan()

        # outer span length
        ospan_length = abs(ospan[1] - ospan[0])

        # if no variant length (such as in the case of a non-deletion variant)
        # default to mean plus 3 stdev
        z = 3
        if var_length is None:
            var_length = self.lib.mean + self.lib.sd * z

        try:
            p = float(self.lib.dens[ospan_length]) * conc_prior / (conc_prior * self.lib.dens[ospan_length] + disc_prior * (self.lib.dens[ospan_length - var_length]))
        except ZeroDivisionError:
            p = None
        return p

def gather_reads(sample,
                 chrom, pos, ci,
                 z,
                 fragment_dict):

    # the distance to the left and right of the breakpoint to scan
    # (max of mean + z standard devs over all of a sample's libraries)
    fetch_flank = sample.get_fetch_flank(z)
    chrom_length = sample.bam.lengths[sample.bam.gettid(chrom)]

    for read in sample.bam.fetch(chrom, max(ci[0] - fetch_flank, 0), min(ci[1] + fetch_flank, chrom_length)):
        lib = sample.get_lib(read.get_tag('RG')) # get the read's library
        if (read.is_unmapped
            or read.is_duplicate
            or lib not in sample.active_libs):
            continue

        # read.query_sequence = "*"
        # read.query_qualities = "*"

        if read.query_name in fragment_dict:
            fragment_dict[read.query_name].add_read(read)
        else:
            fragment_dict[read.query_name] = SamFragment(read, lib)

    return fragment_dict

# primary function
def sv_genotype(bam_string,
                vcf_file,
                vcf_out,
                min_aligned,
                split_weight,
                disc_weight,
                num_samp,
                lib_info_file,
                debug):

    # grant ability to modify global variables
    global out_bam_written_reads

    # parse the comma separated inputs
    bam_list = [pysam.AlignmentFile(b, 'rb') for b in bam_string.split(',')]

    min_lib_prevalence = 1e-3 # only consider libraries that constitute at least this fraction of the BAM

    # parse lib_info_file JSON
    if lib_info_file is not None:
        lib_info = json.load(lib_info_file)

    sample_list = list()
    for i in xrange(len(bam_list)):
        if lib_info_file is None:
            sample = Sample.from_bam(bam_list[i], num_samp, min_lib_prevalence)
        else:
            sample = Sample.from_lib_info(bam_list[i], lib_info, min_lib_prevalence)

        sample_list.append(sample)
    
    # write the JSON for the library
    if lib_info_dump is not None:
        write_sample_json(sample_list, lib_info_dump)

    z = 3
    split_slop = 3 # amount of slop around breakpoint to count splitters
    in_header = True
    header = []
    breakend_dict = {} # cache to hold unmatched generic breakends for genotyping
    vcf = Vcf()

    # read input VCF
    for line in vcf_file:
        if in_header:
            if line[0] == '#':
                header.append(line)
                if line[1] != '#':
                    vcf_samples = line.rstrip().split('\t')[9:]
                continue
            else:
                in_header = False
                vcf.add_header(header)
                # if detailed:
                vcf.add_format('GQ', 1, 'Integer', 'Genotype quality')
                vcf.add_format('SQ', 1, 'Float', 'Phred-scaled probability that this site is variant (non-reference in this sample')
                vcf.add_format('GL', 'G', 'Float', 'Genotype Likelihood, log10-scaled likelihoods of the data given the called genotype for each possible genotype generated from the reference and alternate alleles given the sample ploidy')
                vcf.add_format('DP', 1, 'Integer', 'Read depth')
                vcf.add_format('RO', 1, 'Integer', 'Reference allele observation count, with partial observations recorded fractionally')
                vcf.add_format('AO', 'A', 'Integer', 'Alternate allele observations, with partial observations recorded fractionally')
                vcf.add_format('QR', 1, 'Integer', 'Sum of quality of reference observations')
                vcf.add_format('QA', 'A', 'Integer', 'Sum of quality of alternate observations')
                vcf.add_format('RS', 1, 'Integer', 'Reference allele split-read observation count, with partial observations recorded fractionally')
                vcf.add_format('AS', 'A', 'Integer', 'Alternate allele split-read observation count, with partial observations recorded fractionally')
                vcf.add_format('RP', 1, 'Integer', 'Reference allele paired-end observation count, with partial observations recorded fractionally')
                vcf.add_format('AP', 'A', 'Integer', 'Alternate allele paired-end observation count, with partial observations recorded fractionally')
                vcf.add_format('AB', 'A', 'Float', 'Allele balance, fraction of observations from alternate allele, QA/(QR+QA)')


                # add the samples in the BAM files to the VCF output
                for sample in sample_list:
                    if sample.name not in vcf.sample_list:
                        vcf.add_sample(sample.name)

                # write the output header
                vcf_out.write(vcf.get_header() + '\n')


        v = line.rstrip().split('\t')
        var = Variant(v, vcf)

        # genotype generic breakends
        svtype = var.get_info('SVTYPE')
        if svtype == 'BND':
            if var.info['MATEID'] in breakend_dict:
                var2 = var
                var = breakend_dict[var.info['MATEID']]
                chromA = var.chrom
                chromB = var2.chrom
                posA = var.pos
                posB = var2.pos
                # confidence intervals
                ciA = [posA + ci for ci in map(int, var.info['CIPOS'].split(','))]
                ciB = [posB + ci for ci in map(int, var2.info['CIPOS'].split(','))]

                # infer the strands from the alt allele
                if var.alt[-1] == '[' or var.alt[-1] == ']':
                    o1 = True
                else: o1 = False
                if var2.alt[-1] == '[' or var2.alt[-1] == ']':
                    o2 = True
                else: o2 = False
            else:
                breakend_dict[var.var_id] = var
                continue
        else:
            chromA = var.chrom
            chromB = var.chrom
            posA = var.pos
            posB = int(var.get_info('END'))
            # confidence intervals
            ciA = [posA + ci for ci in map(int, var.info['CIPOS'].split(','))]
            ciB = [posB + ci for ci in map(int, var.info['CIEND'].split(','))]
            if svtype == 'DEL':
                o1, o2 =  True, False
            elif svtype == 'DUP':
                o1, o2 =  False, True
            elif svtype == 'INV':
                o1, o2 = True, True

        # increment the negative strand values (note position in VCF should be the base immediately left of the breakpoint junction)
        if not o1: posA += 1
        if not o2: posB += 1
        # if debug: print posA, posB

        var_length = None
        if svtype == 'DEL':
            var_length = posB - posA

        # print chromA, posA, chromB, posB, svtype

        def clipped_strand(read):
            # clipped on right side
            if read.cigartuples[0][0] in (0, 7) and read.cigartuples[-1][0] in (4, 5):
                return True
            elif read.cigartuples[-1][0] in (0, 7) and read.cigartuples[0][0] in (4, 5):
                return False
            else:
                return None


        def get_left_clip(read):
            clip = 0
            for x in read.cigartuples:
                if x[0] == 4:
                    clip += x[1]
                else:
                    break
            return clip

        def get_right_clip(read):
            clip = 0
            for x in reversed(read.cigartuples):
                if x[0] == 4:
                    clip += x[1]
                else:
                    break
            return clip

        for sample in sample_list:
            read_batch = {}
            read_batch = gather_reads(sample, chromA, posA, ciA, z, read_batch)
            read_batch = gather_reads(sample, chromB, posB, ciB, z, read_batch)
            
            ref_span, alt_span = 0, 0
            ref_seq, alt_seq = 0, 0

            for fragment in read_batch.values():
                ref_ci = [-100,100]


                # 1. Assess splitters
                for read in fragment.primary_reads + fragment.auxil_reads:
                    # # print read.query_alignment_start, read.query_alignment_end
                    # print 
                    # print 'my', get_left_clip(read), get_right_clip(read)
                    # query_alignment_length = fragment.lib.read_length - get_left_clip(read) - get_right_clip(read)
                    # print query_alignment_length
                    # print read.cigartuples

                    left_clip = get_left_clip(read)
                    right_clip = get_right_clip(read)
                    query_alignment_length = fragment.lib.read_length - left_clip - right_clip
                    
                    is_split = False
                    is_ref = False

                    if query_alignment_length < min_aligned:
                        continue

                    if ((read.reference_start <= posA - min_aligned / 2
                         and read.reference_end >= posA + min_aligned / 2)
                        or (read.reference_start <= posB - min_aligned / 2
                            and read.reference_end >= posB + min_aligned / 2)):
                        is_ref = True


                    if right_clip > min_aligned:
                        if ((o1
                             and read.reference_end >= posA - split_slop
                             and read.reference_end <= posA + split_slop)
                            or
                            (o2
                             and read.reference_end >= posB - split_slop
                             and read.reference_end <= posB + split_slop)):
                            is_split = True
                            
                    elif left_clip > min_aligned:
                        if ((not o1
                             and read.reference_start >= posA - split_slop
                             and read.reference_start <= posA + split_slop)
                            or
                            (not o2
                             and read.reference_start >= posB - split_slop
                             and read.reference_start <= posB + split_slop)):
                            is_split = True
                

                    if is_split:
                        p_alt = prob_mapq(read)
                        alt_seq += p_alt
                    
                    if is_ref:
                        p_reference = prob_mapq(read)
                        ref_seq += p_reference
                    
                # tally spanning alternate pairs
                alt_straddle = fragment.is_pair_straddle(chromA, posA, ref_ci,
                                                         chromB, posB, ref_ci,
                                                         o1, o2,
                                                         min_aligned)


                # check both sides if inversion (perhaps should do this for BND as well?)
                if svtype in ('INV'):
                    alt_straddle_reciprocal = fragment.is_pair_straddle(chromA, posA, ref_ci,
                                                                        chromB, posB, ref_ci,
                                                                        flip_strand(o1),
                                                                        flip_strand(o2),
                                                                        min_aligned)
                else: alt_straddle_reciprocal = False

                if alt_straddle or alt_straddle_reciprocal:
                    if svtype == 'DEL':
                        p_conc = fragment.p_concordant(var_length)
                        if p_conc is not None:
                            p_alt = (1 - p_conc) * prob_mapq(fragment.readA) * prob_mapq(fragment.readB)
                            alt_span += p_alt

                            # since an alt straddler is by definition also a reference straddler,
                            # we can bail out early here to save some time
                            p_reference = p_conc * prob_mapq(fragment.readA) * prob_mapq(fragment.readB)
                            ref_span += p_reference
                            continue
                    else:
                        p_alt = prob_mapq(fragment.readA) * prob_mapq(fragment.readB)
                        alt_span += p_alt


                # tally spanning reference pairs
                ref_straddle_A = fragment.is_pair_straddle(chromA, posA, ref_ci,
                                                           chromA, posA, ref_ci,
                                                           True, False,
                                                           min_aligned)
                ref_straddle_B = fragment.is_pair_straddle(chromB, posB, ref_ci,
                                                           chromB, posB, ref_ci,
                                                           True, False,
                                                 min_aligned)

                if ref_straddle_A or ref_straddle_B:
                    p_conc = fragment.p_concordant(var_length)
                    if p_conc is not None:
                        p_reference = p_conc * prob_mapq(fragment.readA) * prob_mapq(fragment.readB)
                        ref_span += p_reference


            if debug:
                print '--------------------------'
                print 'ref_span:', ref_span
                print 'alt_span:', alt_span
                print 'ref_seq:', ref_seq
                print 'alt_seq:', alt_seq

            continue

            if ref_seq + alt_seq + ref_span + alt_span > 0:
                # get bayesian classifier
                if var.info['SVTYPE'] == "DUP": is_dup = True
                else: is_dup = False
                QR = int(split_weight * ref_seq) + int(disc_weight * ref_span)
                QA = int(split_weight * alt_seq) + int(disc_weight * alt_span)
                gt_lplist = bayes_gt(QR, QA, is_dup)
                gt_idx = gt_lplist.index(max(gt_lplist))

                # print log probabilities of homref, het, homalt
                if debug:
                    print gt_lplist

                # set the overall variant QUAL score and sample specific fields
                var.genotype(sample.name).set_format('GL', ','.join(['%.0f' % x for x in gt_lplist]))
                var.genotype(sample.name).set_format('DP', int(ref_seq + alt_seq + ref_span + alt_span))
                var.genotype(sample.name).set_format('RO', int(ref_seq + ref_span))
                var.genotype(sample.name).set_format('AO', int(alt_seq + alt_span))
                var.genotype(sample.name).set_format('QR', QR)
                var.genotype(sample.name).set_format('QA', QA)
                # if detailed:
                var.genotype(sample.name).set_format('RS', int(ref_seq))
                var.genotype(sample.name).set_format('AS', int(alt_seq))
                var.genotype(sample.name).set_format('RP', int(ref_span))
                var.genotype(sample.name).set_format('AP', int(alt_span))
                try:
                    var.genotype(sample.name).set_format('AB', '%.2g' % (QA / float(QR + QA)))
                except ZeroDivisionError:
                    var.genotype(sample.name).set_format('AB', '.')


                # assign genotypes
                gt_sum = 0
                for gt in gt_lplist:
                    try:
                        gt_sum += 10**gt
                    except OverflowError:
                        gt_sum += 0
                if gt_sum > 0:
                    gt_sum_log = math.log(gt_sum, 10)
                    sample_qual = abs(-10 * (gt_lplist[0] - gt_sum_log)) # phred-scaled probability site is non-reference in this sample
                    if 1 - (10**gt_lplist[gt_idx] / 10**gt_sum_log) == 0:
                        phred_gq = 200
                    else:
                        phred_gq = abs(-10 * math.log(1 - (10**gt_lplist[gt_idx] / 10**gt_sum_log), 10))
                    var.genotype(sample.name).set_format('GQ', int(phred_gq))
                    var.genotype(sample.name).set_format('SQ', sample_qual)
                    var.qual += sample_qual
                    if gt_idx == 1:
                        var.genotype(sample.name).set_format('GT', '0/1')
                    elif gt_idx == 2:
                        var.genotype(sample.name).set_format('GT', '1/1')
                    elif gt_idx == 0:
                        var.genotype(sample.name).set_format('GT', '0/0')
                else:
                    var.genotype(sample.name).set_format('GQ', '.')
                    var.genotype(sample.name).set_format('SQ', '.')
                    var.genotype(sample.name).set_format('GT', './.')
            else:
                var.genotype(sample.name).set_format('GT', './.')
                var.qual = 0
                var.genotype(sample.name).set_format('GQ', '.')
                var.genotype(sample.name).set_format('SQ', '.')
                var.genotype(sample.name).set_format('GL', '.')
                var.genotype(sample.name).set_format('DP', 0)
                var.genotype(sample.name).set_format('AO', 0)
                var.genotype(sample.name).set_format('RO', 0)
                # if detailed:
                var.genotype(sample.name).set_format('AS', 0)
                var.genotype(sample.name).set_format('RS', 0)
                var.genotype(sample.name).set_format('AP', 0)
                var.genotype(sample.name).set_format('RP', 0)
                var.genotype(sample.name).set_format('QR', 0)
                var.genotype(sample.name).set_format('QA', 0)
                var.genotype(sample.name).set_format('AB', '.')

        # after all samples have been processed, write
        vcf_out.write(var.get_var_string() + '\n')
        if var.info['SVTYPE'] == 'BND':
            var2.qual = var.qual
            var2.active_formats = var.active_formats
            var2.genotype = var.genotype
            vcf_out.write(var2.get_var_string() + '\n')
    vcf_out.close()

    return

# --------------------------------------
# main function

def main():
    # parse the command line args
    args = get_args()

    # global legacy
    # legacy = args.legacy

    # globally track the reads written to the BAM files
    global out_bam
    global lib_info_dump
    if args.dump:
        template_bam = pysam.AlignmentFile(args.bam.split(',')[0], 'rb')
        out_bam = pysam.AlignmentFile(args.dump + '.bam', 'wb', template_bam)
        template_bam.close()
        lib_info_dump = open(args.dump + '.lib_info.json', 'w')
    else:
        out_bam = None
        lib_info_dump = None

    global out_bam_written_reads
    out_bam_written_reads = set()

    # call primary function
    sv_genotype(args.bam,
                args.input_vcf,
                args.output_vcf,
                args.min_aligned,
                args.split_weight,
                args.disc_weight,
                args.num_samp,
                args.lib_info_file,
                args.debug)

    # close the files
    args.input_vcf.close()
    if args.dump is not None:
        out_bam.close()
    if args.lib_info_file is not None:
        args.lib_info_file.close()

# initialize the script
if __name__ == '__main__':
    try:
        sys.exit(main())
    except IOError, e:
        if e.errno != 32:  # ignore SIGPIPE
            raise
